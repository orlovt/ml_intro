{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep \n",
    "\n",
    "In this notebook we are going to try to set up the text data in a way most efficient for machine learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings;\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data \n",
    "data = fetch_20newsgroups(subset='all', categories=['comp.graphics', 'sci.med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'sci.med']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['data']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: dyer@spdcc.com (Steve Dyer)\\nSubject: Re: Analgesics with Diuretics\\nOrganization: S.P. Dyer Computer Consulting, Cambridge MA\\n\\nIn article <ofk=lve00WB2AvUktO@andrew.cmu.edu> Lawrence Curcio <lc2b+@andrew.cmu.edu> writes:\\n>I sometimes see OTC preparations for muscle aches/back aches that\\n>combine aspirin with a diuretic.\\n\\nYou certainly do not see OTC preparations advertised as such.\\nThe only such ridiculous concoctions are nostrums for premenstrual\\nsyndrome, ostensibly to treat headache and \"bloating\" simultaneously.\\nThey\\'re worthless.\\n\\n>The idea seems to be to reduce\\n>inflammation by getting rid of fluid. Does this actually work? \\n\\nThat\\'s not the idea, and no, they don\\'t work.\\n\\n-- \\nSteve Dyer\\ndyer@ursa-major.spdcc.com aka {ima,harvard,rayssd,linus,m2c}!spdcc!dyer\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample text\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.med'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target for the sample text\n",
    "data['target_names'][target[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split \n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, target, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG-of-WORDS\n",
    "The most native way to generate feature discribtion of the texts would be `vectorization`. Let's say that we have a collection of texts $D = \\{d_i\\}_{i=1}^l$ and a dictionary of all words from our sample space $V = \\{v_j\\}_{j=1}^d$. In this case some text $d_i$ can be described as a vector $(x_{ij})_{j=1}^d$, where $\\large{x_{ij} = \\sum_{v \\in d_i}[v= v_j]}$. This would imply that a given text $d_i$ is described by a vector of a number of ~mentions~ in the collection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(encoding='utf8')\n",
    "_ = vectorizer.fit(texts_train)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x29227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 43 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  451  3280  3285  4133  4134  5214  5848  7692  8797  9094  9438  9616\n",
      "  9904 11526 12024 12364 13304 13697 14400 14592 14944 14950 16427 17425\n",
      " 18814 19241 19490 20697 20955 20984 21019 21280 21693 21990 22035 22535\n",
      " 23585 25311 26187 26499 26956 28495 28772]\n",
      "[1 1 1 2 2 3 1 1 4 1 1 1 5 2 1 1 1 1 4 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 5 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(texts_train[:1]).indices)\n",
    "print(vectorizer.transform(texts_train[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search we fill find optimal hyperparameters. To use the binary classification we will use the accuarcy as target metric. \n",
    "\n",
    "$$Accuracy(\\hat{y}, y) = \\frac{1}{N}\\sum_{i=1}^{N}[\\hat{y}=y]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training func\n",
    "def train_model(X_train, y_train):\n",
    "    alphas = np.logspace(-1, 3, 10)\n",
    "    searcher = GridSearchCV(LogisticRegression(), [{'C': alphas, 'max_iter': [500]}],\n",
    "                            scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    searcher.fit(X_train, y_train)\n",
    "\n",
    "    best_alpha = searcher.best_params_[\"C\"]\n",
    "    print(\"Best alpha = %.4f\" % best_alpha)\n",
    "    \n",
    "    model = LogisticRegression(C=best_alpha, max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transfromation \n",
    "X_train = vectorizer.transform(texts_train)\n",
    "X_test = vectorizer.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 0.7743\n",
      "Train accuracy = 1.0000\n",
      "Test accuracy = 0.9720\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n",
    "print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF\n",
    "\n",
    "Another way of struckturing data for this scenario is [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (**T**erm **F**requency–**I**nverse **D**ocument **F**requency). \n",
    "\n",
    "- $D$ is the collection of texts, where $t$ is the frequency of a unique word in text $d \\in D$\n",
    "- TermFrequency - proportion of the word in text $\\large{tf(t, d) = \\frac{n_{td}}{\\sum n_{td}}}$\n",
    "- InverseDocumentFrequency $\\large{idf(t,d) = log\\frac{|D|}{|\\{d \\in D:t \\in d\\}|}}$, where $|\\{d \\in D:t \\in d\\}|$ i the number of texts in the collection containing word $t$. \n",
    "\n",
    "Then for every word, text $(t,d)$ pair we can compute ${tf{\\text -}idf(t,d,D) = tf(t,d) \\times idf(t, D)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29227"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(encoding='utf8')\n",
    "_ = vectorizer.fit(texts_train)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x29227 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 43 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29097 28831 28793 28751 28665 27566 26802 26499 26318 26283 26211 26209\n",
      " 25758 25396 25311 25072 24692 24588 24253 23753 23743 22882 22873 22227\n",
      " 22035 22002 21137 21124 19566 19553 19490 19356 19263 19241 18937 18935\n",
      " 18816 18280 16947 16821 16812 16775 16445 16140 16116 14432 14253 14120\n",
      " 14015 13218 13150 12376 12024 11805 11721 10241 10080  9773  9741  9712\n",
      "  9632  9631  8497  7916  7743  7692  7482  7473  7316  6729  6385  6167\n",
      "  5606  5186  4963  4616  4575  4555  4461  4138  4127  4097  3818  3633\n",
      "  3486  3419]\n",
      "[0.02847645 0.03142552 0.10526438 0.10312729 0.05644174 0.08756307\n",
      " 0.07828735 0.05600843 0.02587729 0.07735152 0.05526081 0.04728733\n",
      " 0.07421514 0.09664853 0.01676178 0.123904   0.25103298 0.06939308\n",
      " 0.09992653 0.05728882 0.10066724 0.1015241  0.09846807 0.08101145\n",
      " 0.05042139 0.08830816 0.23376552 0.12170482 0.18525966 0.12170482\n",
      " 0.01735912 0.04445822 0.12170482 0.01877742 0.05769067 0.12170482\n",
      " 0.03931898 0.08550803 0.06326732 0.07149103 0.08830816 0.12170482\n",
      " 0.08255979 0.10526438 0.09076549 0.08756307 0.02039305 0.08830816\n",
      " 0.13733887 0.08908792 0.07094142 0.06163869 0.01676178 0.04409804\n",
      " 0.09846807 0.04937804 0.48032027 0.03977595 0.03948371 0.03500662\n",
      " 0.11314248 0.11688276 0.10329013 0.08908792 0.12170482 0.04348468\n",
      " 0.09846807 0.06590317 0.16621882 0.06866943 0.08310941 0.03503833\n",
      " 0.12170482 0.02705738 0.05494616 0.10750261 0.03127267 0.03221137\n",
      " 0.02891128 0.15829423 0.04023415 0.10526438 0.08101145 0.11008645\n",
      " 0.06015437 0.23376552]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(texts[:1]).indices)\n",
    "print(vectorizer.transform(texts[:1]).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 1000.0000\n",
      "Train accuracy = 1.0000\n",
      "Test accuracy = 0.9873\n"
     ]
    }
   ],
   "source": [
    "# transform data and find optimal hyperparam \n",
    "\n",
    "X_train = vectorizer.transform(texts_train)\n",
    "X_test = vectorizer.transform(texts_test)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n",
    "print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization \n",
    "\n",
    "- [Stemming](https://en.wikipedia.org/wiki/Stemming) - For the skiing technique, see Stem (skiing). For the climbing technique, see Glossary of climbing terms § stem.\n",
    "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "\n",
    "- [Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) - is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a stemming lib \n",
    "import nltk\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vizual\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "print(stemmer.stem('vizuallization'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try **stemming** for the preprocessing of thr text before **vectorization**. We will use **td-idf** for vectorization, because it has shows the best results before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4032fc9cead4f8b867edc83fc0b2afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb6624573744d22b5e33fbd308a9c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def stem_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(map(lambda w: stemmer.stem(w), tokens))\n",
    "\n",
    "stemmed_texts_train = []\n",
    "for t in tqdm(texts_train):\n",
    "    stemmed_texts_train.append(stem_text(t, stemmer))\n",
    "    \n",
    "stemmed_texts_test = []\n",
    "for t in tqdm(texts_test):\n",
    "    stemmed_texts_test.append(stem_text(t, stemmer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the initial and stemmed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: black@sybase.com (Chris Black)\n",
      "Subject: cystic breast disease\n",
      "Organization: Sybase, Inc.\n",
      "Lines: 18\n",
      "\n",
      "My mom has just been diagnosed with cystic breast disease -- a big\n",
      "relief, as it was a lump that could have been cancer.  Her doctor says\n",
      "she should go off caffeine and chocolate for 6 months, as well as\n",
      "stopping the estrogen she's been taking for menopause-related reasons.\n",
      "She's not thrilled with this, I think especially because she just gave\n",
      "up cigarettes -- soon she won't have any pleasures left!  Now, I thought\n",
      "I'd heard that cystic breasts were common and not really a health risk.\n",
      "Is this accurate?  If so, why is she being told to make various\n",
      "sacrifices to treat something that's not that big of a deal?\n",
      "\n",
      "Thanks for any information.\n",
      "\n",
      "-- Chris\n",
      "\n",
      "-- \n",
      "black@sybase.com\n",
      "\n",
      "Note:  My mailer tends to garble subject lines.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: black@sybase.com (chris black) subject: cystic breast diseas organization: sybase, inc. lines: 18 my mom has just been diagnos with cystic breast diseas -- a big relief, as it was a lump that could have been cancer. her doctor say she should go off caffein and chocol for 6 months, as well as stop the estrogen she been take for menopause-rel reasons. she not thrill with this, i think especi becaus she just gave up cigarett -- soon she won't have ani pleasur left! now, i thought i'd heard that cystic breast were common and not realli a health risk. is this accurate? if so, whi is she be told to make various sacrific to treat someth that not that big of a deal? thank for ani information. -- chris -- black@sybase.com note: my mailer tend to garbl subject lines.\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_texts_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8')\n",
    "_ = vectorizer.fit(stemmed_texts_train)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(stemmed_texts_train)\n",
    "X_test = vectorizer.transform(stemmed_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 129.1550\n",
      "Train accuracy = 1.0000\n",
      "Test accuracy = 0.9924\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n",
    "print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07de20fcdc0f46de8c4e19d20a8f5c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bfdfffb234489e87315082124d57bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(map(lambda w: lemmatizer.lemmatize(w), tokens))\n",
    "\n",
    "lemmatized_texts_train = []\n",
    "for t in tqdm(texts_train):\n",
    "    lemmatized_texts_train.append(lemmatize_text(t, stemmer))\n",
    "    \n",
    "lemmatized_texts_test = []\n",
    "for t in tqdm(texts_test):\n",
    "    lemmatized_texts_test.append(lemmatize_text(t, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: black@sybase.com (Chris Black) Subject: cystic breast disease Organization: Sybase, Inc. Lines: 18 My mom ha just been diagnosed with cystic breast disease -- a big relief, a it wa a lump that could have been cancer. Her doctor say she should go off caffeine and chocolate for 6 months, a well a stopping the estrogen she's been taking for menopause-related reasons. She's not thrilled with this, I think especially because she just gave up cigarette -- soon she won't have any pleasure left! Now, I thought I'd heard that cystic breast were common and not really a health risk. Is this accurate? If so, why is she being told to make various sacrifice to treat something that's not that big of a deal? Thanks for any information. -- Chris -- black@sybase.com Note: My mailer tends to garble subject lines.\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_texts_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: black@sybase.com (chris black) subject: cystic breast diseas organization: sybase, inc. lines: 18 my mom has just been diagnos with cystic breast diseas -- a big relief, as it was a lump that could have been cancer. her doctor say she should go off caffein and chocol for 6 months, as well as stop the estrogen she been take for menopause-rel reasons. she not thrill with this, i think especi becaus she just gave up cigarett -- soon she won't have ani pleasur left! now, i thought i'd heard that cystic breast were common and not realli a health risk. is this accurate? if so, whi is she be told to make various sacrific to treat someth that not that big of a deal? thank for ani information. -- chris -- black@sybase.com note: my mailer tend to garbl subject lines.\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_texts_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28717"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8')\n",
    "_ = vectorizer.fit(lemmatized_texts_train)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(lemmatized_texts_train)\n",
    "X_test = vectorizer.transform(lemmatized_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 359.3814\n",
      "Train accuracy = 1.0000\n",
      "Test accuracy = 0.9873\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n",
    "print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- BAG-of-WORDS       Test accuracy = 0.9720 \n",
    "- TD-IDF             Test accuracy = 0.9873\n",
    "\n",
    "We can see that td-idf has shown to be more accurate, this can be contributed to the nature of the model, which allows us to see the **more significant** words. \n",
    "\n",
    "\n",
    "Next we've used two approaches to the text preprocessing with the following results: \n",
    "\n",
    "- Lemmatization     Test accuracy = 0.9924\n",
    "- Stemming          Test accuracy = 0.9873"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "299788cdfb4ccb82fb4a8515b6fdc57e1f1020095d820d2e6ce895ac7909e7df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
